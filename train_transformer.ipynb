{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a14d7b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Make a test set.\n",
    "- Make more data.\n",
    "- Look into what methods are good for using a transformer to predict an embedding. (we aren't doing softmax style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423192a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ml\\mtg deckbuilding\\new\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas torch\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41b932c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: torch.Size([101, 23, 256])\n",
      "targets shape: torch.Size([101, 256])\n",
      "Number of input sequences: 101\n",
      "Number of target vectors: 101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),\"transformer_training_data\", \"ONE\", \"train_data.pt\")\n",
    "inputs, targets = torch.load(data_path)\n",
    "print(f\"inputs shape: {inputs.shape}\")\n",
    "print(f\"targets shape: {targets.shape}\")\n",
    "print(f\"Number of input sequences: {len(inputs)}\")\n",
    "print(f\"Number of target vectors: {len(targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de43a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BasicTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=256, seq_len=100, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.output = nn.Linear(embed_dim, embed_dim)  # output is a 256-dim embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # Pool over sequence dimension\n",
    "        x = x.transpose(1, 2)  # (batch, embed_dim, seq_len)\n",
    "        x = self.pool(x).squeeze(-1)  # (batch, embed_dim)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# model = BasicTransformer(input_dim=inputs.shape[2])\n",
    "# output = model(inputs[:, :100, :])  # limit sequence length to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f686245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.3103\n",
      "Epoch 2/500, Loss: 0.4006\n",
      "Epoch 3/500, Loss: 0.1473\n",
      "Epoch 4/500, Loss: 0.0683\n",
      "Epoch 5/500, Loss: 0.0423\n",
      "Epoch 6/500, Loss: 0.0332\n",
      "Epoch 7/500, Loss: 0.0332\n",
      "Epoch 8/500, Loss: 0.0353\n",
      "Epoch 9/500, Loss: 0.0348\n",
      "Epoch 10/500, Loss: 0.0312\n",
      "Epoch 11/500, Loss: 0.0264\n",
      "Epoch 12/500, Loss: 0.0219\n",
      "Epoch 13/500, Loss: 0.0184\n",
      "Epoch 14/500, Loss: 0.0162\n",
      "Epoch 15/500, Loss: 0.0147\n",
      "Epoch 16/500, Loss: 0.0141\n",
      "Epoch 17/500, Loss: 0.0138\n",
      "Epoch 18/500, Loss: 0.0137\n",
      "Epoch 19/500, Loss: 0.0138\n",
      "Epoch 20/500, Loss: 0.0138\n",
      "Epoch 21/500, Loss: 0.0137\n",
      "Epoch 22/500, Loss: 0.0131\n",
      "Epoch 23/500, Loss: 0.0123\n",
      "Epoch 24/500, Loss: 0.0112\n",
      "Epoch 25/500, Loss: 0.0104\n",
      "Epoch 26/500, Loss: 0.0100\n",
      "Epoch 27/500, Loss: 0.0099\n",
      "Epoch 28/500, Loss: 0.0100\n",
      "Epoch 29/500, Loss: 0.0102\n",
      "Epoch 30/500, Loss: 0.0102\n",
      "Epoch 31/500, Loss: 0.0102\n",
      "Epoch 32/500, Loss: 0.0100\n",
      "Epoch 33/500, Loss: 0.0096\n",
      "Epoch 34/500, Loss: 0.0094\n",
      "Epoch 35/500, Loss: 0.0091\n",
      "Epoch 36/500, Loss: 0.0090\n",
      "Epoch 37/500, Loss: 0.0090\n",
      "Epoch 38/500, Loss: 0.0090\n",
      "Epoch 39/500, Loss: 0.0089\n",
      "Epoch 40/500, Loss: 0.0088\n",
      "Epoch 41/500, Loss: 0.0087\n",
      "Epoch 42/500, Loss: 0.0087\n",
      "Epoch 43/500, Loss: 0.0087\n",
      "Epoch 44/500, Loss: 0.0086\n",
      "Epoch 45/500, Loss: 0.0085\n",
      "Epoch 46/500, Loss: 0.0085\n",
      "Epoch 47/500, Loss: 0.0084\n",
      "Epoch 48/500, Loss: 0.0083\n",
      "Epoch 49/500, Loss: 0.0083\n",
      "Epoch 50/500, Loss: 0.0082\n",
      "Epoch 51/500, Loss: 0.0081\n",
      "Epoch 52/500, Loss: 0.0082\n",
      "Epoch 53/500, Loss: 0.0081\n",
      "Epoch 54/500, Loss: 0.0080\n",
      "Epoch 55/500, Loss: 0.0080\n",
      "Epoch 56/500, Loss: 0.0080\n",
      "Epoch 57/500, Loss: 0.0078\n",
      "Epoch 58/500, Loss: 0.0079\n",
      "Epoch 59/500, Loss: 0.0078\n",
      "Epoch 60/500, Loss: 0.0077\n",
      "Epoch 61/500, Loss: 0.0077\n",
      "Epoch 62/500, Loss: 0.0077\n",
      "Epoch 63/500, Loss: 0.0078\n",
      "Epoch 64/500, Loss: 0.0077\n",
      "Epoch 65/500, Loss: 0.0076\n",
      "Epoch 66/500, Loss: 0.0076\n",
      "Epoch 67/500, Loss: 0.0075\n",
      "Epoch 68/500, Loss: 0.0075\n",
      "Epoch 69/500, Loss: 0.0075\n",
      "Epoch 70/500, Loss: 0.0074\n",
      "Epoch 71/500, Loss: 0.0074\n",
      "Epoch 72/500, Loss: 0.0074\n",
      "Epoch 73/500, Loss: 0.0074\n",
      "Epoch 74/500, Loss: 0.0073\n",
      "Epoch 75/500, Loss: 0.0073\n",
      "Epoch 76/500, Loss: 0.0072\n",
      "Epoch 77/500, Loss: 0.0071\n",
      "Epoch 78/500, Loss: 0.0072\n",
      "Epoch 79/500, Loss: 0.0072\n",
      "Epoch 80/500, Loss: 0.0071\n",
      "Epoch 81/500, Loss: 0.0071\n",
      "Epoch 82/500, Loss: 0.0071\n",
      "Epoch 83/500, Loss: 0.0070\n",
      "Epoch 84/500, Loss: 0.0071\n",
      "Epoch 85/500, Loss: 0.0070\n",
      "Epoch 86/500, Loss: 0.0070\n",
      "Epoch 87/500, Loss: 0.0069\n",
      "Epoch 88/500, Loss: 0.0069\n",
      "Epoch 89/500, Loss: 0.0069\n",
      "Epoch 90/500, Loss: 0.0069\n",
      "Epoch 91/500, Loss: 0.0069\n",
      "Epoch 92/500, Loss: 0.0069\n",
      "Epoch 93/500, Loss: 0.0068\n",
      "Epoch 94/500, Loss: 0.0068\n",
      "Epoch 95/500, Loss: 0.0068\n",
      "Epoch 96/500, Loss: 0.0068\n",
      "Epoch 97/500, Loss: 0.0067\n",
      "Epoch 98/500, Loss: 0.0067\n",
      "Epoch 99/500, Loss: 0.0067\n",
      "Epoch 100/500, Loss: 0.0067\n",
      "Epoch 101/500, Loss: 0.0068\n",
      "Epoch 102/500, Loss: 0.0066\n",
      "Epoch 103/500, Loss: 0.0066\n",
      "Epoch 104/500, Loss: 0.0066\n",
      "Epoch 105/500, Loss: 0.0066\n",
      "Epoch 106/500, Loss: 0.0065\n",
      "Epoch 107/500, Loss: 0.0066\n",
      "Epoch 108/500, Loss: 0.0065\n",
      "Epoch 109/500, Loss: 0.0065\n",
      "Epoch 110/500, Loss: 0.0065\n",
      "Epoch 111/500, Loss: 0.0065\n",
      "Epoch 112/500, Loss: 0.0065\n",
      "Epoch 113/500, Loss: 0.0065\n",
      "Epoch 114/500, Loss: 0.0065\n",
      "Epoch 115/500, Loss: 0.0065\n",
      "Epoch 116/500, Loss: 0.0066\n",
      "Epoch 117/500, Loss: 0.0064\n",
      "Epoch 118/500, Loss: 0.0064\n",
      "Epoch 119/500, Loss: 0.0064\n",
      "Epoch 120/500, Loss: 0.0064\n",
      "Epoch 121/500, Loss: 0.0064\n",
      "Epoch 122/500, Loss: 0.0064\n",
      "Epoch 123/500, Loss: 0.0064\n",
      "Epoch 124/500, Loss: 0.0064\n",
      "Epoch 125/500, Loss: 0.0064\n",
      "Epoch 126/500, Loss: 0.0064\n",
      "Epoch 127/500, Loss: 0.0064\n",
      "Epoch 128/500, Loss: 0.0064\n",
      "Epoch 129/500, Loss: 0.0064\n",
      "Epoch 130/500, Loss: 0.0063\n",
      "Epoch 131/500, Loss: 0.0063\n",
      "Epoch 132/500, Loss: 0.0063\n",
      "Epoch 133/500, Loss: 0.0063\n",
      "Epoch 134/500, Loss: 0.0064\n",
      "Epoch 135/500, Loss: 0.0063\n",
      "Epoch 136/500, Loss: 0.0063\n",
      "Epoch 137/500, Loss: 0.0063\n",
      "Epoch 138/500, Loss: 0.0063\n",
      "Epoch 139/500, Loss: 0.0062\n",
      "Epoch 140/500, Loss: 0.0062\n",
      "Epoch 141/500, Loss: 0.0062\n",
      "Epoch 142/500, Loss: 0.0063\n",
      "Epoch 143/500, Loss: 0.0062\n",
      "Epoch 144/500, Loss: 0.0062\n",
      "Epoch 145/500, Loss: 0.0062\n",
      "Epoch 146/500, Loss: 0.0062\n",
      "Epoch 147/500, Loss: 0.0062\n",
      "Epoch 148/500, Loss: 0.0062\n",
      "Epoch 149/500, Loss: 0.0062\n",
      "Epoch 150/500, Loss: 0.0061\n",
      "Epoch 151/500, Loss: 0.0061\n",
      "Epoch 152/500, Loss: 0.0062\n",
      "Epoch 153/500, Loss: 0.0061\n",
      "Epoch 154/500, Loss: 0.0062\n",
      "Epoch 155/500, Loss: 0.0061\n",
      "Epoch 156/500, Loss: 0.0061\n",
      "Epoch 157/500, Loss: 0.0061\n",
      "Epoch 158/500, Loss: 0.0061\n",
      "Epoch 159/500, Loss: 0.0061\n",
      "Epoch 160/500, Loss: 0.0061\n",
      "Epoch 161/500, Loss: 0.0061\n",
      "Epoch 162/500, Loss: 0.0061\n",
      "Epoch 163/500, Loss: 0.0061\n",
      "Epoch 164/500, Loss: 0.0060\n",
      "Epoch 165/500, Loss: 0.0061\n",
      "Epoch 166/500, Loss: 0.0060\n",
      "Epoch 167/500, Loss: 0.0061\n",
      "Epoch 168/500, Loss: 0.0061\n",
      "Epoch 169/500, Loss: 0.0060\n",
      "Epoch 170/500, Loss: 0.0060\n",
      "Epoch 171/500, Loss: 0.0060\n",
      "Epoch 172/500, Loss: 0.0060\n",
      "Epoch 173/500, Loss: 0.0060\n",
      "Epoch 174/500, Loss: 0.0059\n",
      "Epoch 175/500, Loss: 0.0060\n",
      "Epoch 176/500, Loss: 0.0060\n",
      "Epoch 177/500, Loss: 0.0060\n",
      "Epoch 178/500, Loss: 0.0060\n",
      "Epoch 179/500, Loss: 0.0059\n",
      "Epoch 180/500, Loss: 0.0059\n",
      "Epoch 181/500, Loss: 0.0060\n",
      "Epoch 182/500, Loss: 0.0059\n",
      "Epoch 183/500, Loss: 0.0059\n",
      "Epoch 184/500, Loss: 0.0059\n",
      "Epoch 185/500, Loss: 0.0059\n",
      "Epoch 186/500, Loss: 0.0059\n",
      "Epoch 187/500, Loss: 0.0059\n",
      "Epoch 188/500, Loss: 0.0059\n",
      "Epoch 189/500, Loss: 0.0059\n",
      "Epoch 190/500, Loss: 0.0058\n",
      "Epoch 191/500, Loss: 0.0059\n",
      "Epoch 192/500, Loss: 0.0058\n",
      "Epoch 193/500, Loss: 0.0059\n",
      "Epoch 194/500, Loss: 0.0059\n",
      "Epoch 195/500, Loss: 0.0058\n",
      "Epoch 196/500, Loss: 0.0058\n",
      "Epoch 197/500, Loss: 0.0058\n",
      "Epoch 198/500, Loss: 0.0058\n",
      "Epoch 199/500, Loss: 0.0059\n",
      "Epoch 200/500, Loss: 0.0058\n",
      "Epoch 201/500, Loss: 0.0058\n",
      "Epoch 202/500, Loss: 0.0057\n",
      "Epoch 203/500, Loss: 0.0058\n",
      "Epoch 204/500, Loss: 0.0057\n",
      "Epoch 205/500, Loss: 0.0058\n",
      "Epoch 206/500, Loss: 0.0058\n",
      "Epoch 207/500, Loss: 0.0057\n",
      "Epoch 208/500, Loss: 0.0058\n",
      "Epoch 209/500, Loss: 0.0057\n",
      "Epoch 210/500, Loss: 0.0057\n",
      "Epoch 211/500, Loss: 0.0057\n",
      "Epoch 212/500, Loss: 0.0057\n",
      "Epoch 213/500, Loss: 0.0056\n",
      "Epoch 214/500, Loss: 0.0057\n",
      "Epoch 215/500, Loss: 0.0058\n",
      "Epoch 216/500, Loss: 0.0057\n",
      "Epoch 217/500, Loss: 0.0057\n",
      "Epoch 218/500, Loss: 0.0057\n",
      "Epoch 219/500, Loss: 0.0057\n",
      "Epoch 220/500, Loss: 0.0057\n",
      "Epoch 221/500, Loss: 0.0057\n",
      "Epoch 222/500, Loss: 0.0057\n",
      "Epoch 223/500, Loss: 0.0057\n",
      "Epoch 224/500, Loss: 0.0056\n",
      "Epoch 225/500, Loss: 0.0056\n",
      "Epoch 226/500, Loss: 0.0056\n",
      "Epoch 227/500, Loss: 0.0056\n",
      "Epoch 228/500, Loss: 0.0056\n",
      "Epoch 229/500, Loss: 0.0056\n",
      "Epoch 230/500, Loss: 0.0056\n",
      "Epoch 231/500, Loss: 0.0056\n",
      "Epoch 232/500, Loss: 0.0055\n",
      "Epoch 233/500, Loss: 0.0056\n",
      "Epoch 234/500, Loss: 0.0057\n",
      "Epoch 235/500, Loss: 0.0062\n",
      "Epoch 236/500, Loss: 0.0060\n",
      "Epoch 237/500, Loss: 0.0055\n",
      "Epoch 238/500, Loss: 0.0058\n",
      "Epoch 239/500, Loss: 0.0057\n",
      "Epoch 240/500, Loss: 0.0056\n",
      "Epoch 241/500, Loss: 0.0057\n",
      "Epoch 242/500, Loss: 0.0055\n",
      "Epoch 243/500, Loss: 0.0056\n",
      "Epoch 244/500, Loss: 0.0055\n",
      "Epoch 245/500, Loss: 0.0056\n",
      "Epoch 246/500, Loss: 0.0056\n",
      "Epoch 247/500, Loss: 0.0055\n",
      "Epoch 248/500, Loss: 0.0055\n",
      "Epoch 249/500, Loss: 0.0054\n",
      "Epoch 250/500, Loss: 0.0055\n",
      "Epoch 251/500, Loss: 0.0055\n",
      "Epoch 252/500, Loss: 0.0054\n",
      "Epoch 253/500, Loss: 0.0054\n",
      "Epoch 254/500, Loss: 0.0055\n",
      "Epoch 255/500, Loss: 0.0054\n",
      "Epoch 256/500, Loss: 0.0053\n",
      "Epoch 257/500, Loss: 0.0053\n",
      "Epoch 258/500, Loss: 0.0054\n",
      "Epoch 259/500, Loss: 0.0054\n",
      "Epoch 260/500, Loss: 0.0054\n",
      "Epoch 261/500, Loss: 0.0053\n",
      "Epoch 262/500, Loss: 0.0054\n",
      "Epoch 263/500, Loss: 0.0054\n",
      "Epoch 264/500, Loss: 0.0053\n",
      "Epoch 265/500, Loss: 0.0053\n",
      "Epoch 266/500, Loss: 0.0053\n",
      "Epoch 267/500, Loss: 0.0052\n",
      "Epoch 268/500, Loss: 0.0052\n",
      "Epoch 269/500, Loss: 0.0053\n",
      "Epoch 270/500, Loss: 0.0055\n",
      "Epoch 271/500, Loss: 0.0055\n",
      "Epoch 272/500, Loss: 0.0055\n",
      "Epoch 273/500, Loss: 0.0055\n",
      "Epoch 274/500, Loss: 0.0054\n",
      "Epoch 275/500, Loss: 0.0057\n",
      "Epoch 276/500, Loss: 0.0054\n",
      "Epoch 277/500, Loss: 0.0054\n",
      "Epoch 278/500, Loss: 0.0054\n",
      "Epoch 279/500, Loss: 0.0054\n",
      "Epoch 280/500, Loss: 0.0054\n",
      "Epoch 281/500, Loss: 0.0054\n",
      "Epoch 282/500, Loss: 0.0053\n",
      "Epoch 283/500, Loss: 0.0052\n",
      "Epoch 284/500, Loss: 0.0052\n",
      "Epoch 285/500, Loss: 0.0052\n",
      "Epoch 286/500, Loss: 0.0052\n",
      "Epoch 287/500, Loss: 0.0053\n",
      "Epoch 288/500, Loss: 0.0051\n",
      "Epoch 289/500, Loss: 0.0051\n",
      "Epoch 290/500, Loss: 0.0051\n",
      "Epoch 291/500, Loss: 0.0051\n",
      "Epoch 292/500, Loss: 0.0051\n",
      "Epoch 293/500, Loss: 0.0052\n",
      "Epoch 294/500, Loss: 0.0051\n",
      "Epoch 295/500, Loss: 0.0051\n",
      "Epoch 296/500, Loss: 0.0052\n",
      "Epoch 297/500, Loss: 0.0052\n",
      "Epoch 298/500, Loss: 0.0052\n",
      "Epoch 299/500, Loss: 0.0050\n",
      "Epoch 300/500, Loss: 0.0050\n",
      "Epoch 301/500, Loss: 0.0052\n",
      "Epoch 302/500, Loss: 0.0052\n",
      "Epoch 303/500, Loss: 0.0050\n",
      "Epoch 304/500, Loss: 0.0049\n",
      "Epoch 305/500, Loss: 0.0050\n",
      "Epoch 306/500, Loss: 0.0050\n",
      "Epoch 307/500, Loss: 0.0051\n",
      "Epoch 308/500, Loss: 0.0050\n",
      "Epoch 309/500, Loss: 0.0049\n",
      "Epoch 310/500, Loss: 0.0049\n",
      "Epoch 311/500, Loss: 0.0050\n",
      "Epoch 312/500, Loss: 0.0050\n",
      "Epoch 313/500, Loss: 0.0049\n",
      "Epoch 314/500, Loss: 0.0048\n",
      "Epoch 315/500, Loss: 0.0048\n",
      "Epoch 316/500, Loss: 0.0047\n",
      "Epoch 317/500, Loss: 0.0048\n",
      "Epoch 318/500, Loss: 0.0050\n",
      "Epoch 319/500, Loss: 0.0051\n",
      "Epoch 320/500, Loss: 0.0055\n",
      "Epoch 321/500, Loss: 0.0059\n",
      "Epoch 322/500, Loss: 0.0049\n",
      "Epoch 323/500, Loss: 0.0049\n",
      "Epoch 324/500, Loss: 0.0053\n",
      "Epoch 325/500, Loss: 0.0048\n",
      "Epoch 326/500, Loss: 0.0052\n",
      "Epoch 327/500, Loss: 0.0049\n",
      "Epoch 328/500, Loss: 0.0055\n",
      "Epoch 329/500, Loss: 0.0049\n",
      "Epoch 330/500, Loss: 0.0058\n",
      "Epoch 331/500, Loss: 0.0054\n",
      "Epoch 332/500, Loss: 0.0056\n",
      "Epoch 333/500, Loss: 0.0053\n",
      "Epoch 334/500, Loss: 0.0053\n",
      "Epoch 335/500, Loss: 0.0051\n",
      "Epoch 336/500, Loss: 0.0053\n",
      "Epoch 337/500, Loss: 0.0051\n",
      "Epoch 338/500, Loss: 0.0050\n",
      "Epoch 339/500, Loss: 0.0049\n",
      "Epoch 340/500, Loss: 0.0049\n",
      "Epoch 341/500, Loss: 0.0049\n",
      "Epoch 342/500, Loss: 0.0049\n",
      "Epoch 343/500, Loss: 0.0050\n",
      "Epoch 344/500, Loss: 0.0048\n",
      "Epoch 345/500, Loss: 0.0048\n",
      "Epoch 346/500, Loss: 0.0048\n",
      "Epoch 347/500, Loss: 0.0047\n",
      "Epoch 348/500, Loss: 0.0046\n",
      "Epoch 349/500, Loss: 0.0046\n",
      "Epoch 350/500, Loss: 0.0046\n",
      "Epoch 351/500, Loss: 0.0046\n",
      "Epoch 352/500, Loss: 0.0047\n",
      "Epoch 353/500, Loss: 0.0047\n",
      "Epoch 354/500, Loss: 0.0045\n",
      "Epoch 355/500, Loss: 0.0045\n",
      "Epoch 356/500, Loss: 0.0045\n",
      "Epoch 357/500, Loss: 0.0046\n",
      "Epoch 358/500, Loss: 0.0045\n",
      "Epoch 359/500, Loss: 0.0044\n",
      "Epoch 360/500, Loss: 0.0044\n",
      "Epoch 361/500, Loss: 0.0045\n",
      "Epoch 362/500, Loss: 0.0045\n",
      "Epoch 363/500, Loss: 0.0044\n",
      "Epoch 364/500, Loss: 0.0044\n",
      "Epoch 365/500, Loss: 0.0043\n",
      "Epoch 366/500, Loss: 0.0044\n",
      "Epoch 367/500, Loss: 0.0043\n",
      "Epoch 368/500, Loss: 0.0043\n",
      "Epoch 369/500, Loss: 0.0043\n",
      "Epoch 370/500, Loss: 0.0045\n",
      "Epoch 371/500, Loss: 0.0048\n",
      "Epoch 372/500, Loss: 0.0054\n",
      "Epoch 373/500, Loss: 0.0048\n",
      "Epoch 374/500, Loss: 0.0042\n",
      "Epoch 375/500, Loss: 0.0045\n",
      "Epoch 376/500, Loss: 0.0045\n",
      "Epoch 377/500, Loss: 0.0043\n",
      "Epoch 378/500, Loss: 0.0044\n",
      "Epoch 379/500, Loss: 0.0043\n",
      "Epoch 380/500, Loss: 0.0045\n",
      "Epoch 381/500, Loss: 0.0044\n",
      "Epoch 382/500, Loss: 0.0043\n",
      "Epoch 383/500, Loss: 0.0044\n",
      "Epoch 384/500, Loss: 0.0042\n",
      "Epoch 385/500, Loss: 0.0043\n",
      "Epoch 386/500, Loss: 0.0041\n",
      "Epoch 387/500, Loss: 0.0041\n",
      "Epoch 388/500, Loss: 0.0043\n",
      "Epoch 389/500, Loss: 0.0042\n",
      "Epoch 390/500, Loss: 0.0041\n",
      "Epoch 391/500, Loss: 0.0041\n",
      "Epoch 392/500, Loss: 0.0040\n",
      "Epoch 393/500, Loss: 0.0041\n",
      "Epoch 394/500, Loss: 0.0040\n",
      "Epoch 395/500, Loss: 0.0040\n",
      "Epoch 396/500, Loss: 0.0041\n",
      "Epoch 397/500, Loss: 0.0040\n",
      "Epoch 398/500, Loss: 0.0041\n",
      "Epoch 399/500, Loss: 0.0040\n",
      "Epoch 400/500, Loss: 0.0040\n",
      "Epoch 401/500, Loss: 0.0042\n",
      "Epoch 402/500, Loss: 0.0041\n",
      "Epoch 403/500, Loss: 0.0040\n",
      "Epoch 404/500, Loss: 0.0039\n",
      "Epoch 405/500, Loss: 0.0040\n",
      "Epoch 406/500, Loss: 0.0040\n",
      "Epoch 407/500, Loss: 0.0039\n",
      "Epoch 408/500, Loss: 0.0041\n",
      "Epoch 409/500, Loss: 0.0039\n",
      "Epoch 410/500, Loss: 0.0039\n",
      "Epoch 411/500, Loss: 0.0039\n",
      "Epoch 412/500, Loss: 0.0038\n",
      "Epoch 413/500, Loss: 0.0040\n",
      "Epoch 414/500, Loss: 0.0040\n",
      "Epoch 415/500, Loss: 0.0043\n",
      "Epoch 416/500, Loss: 0.0044\n",
      "Epoch 417/500, Loss: 0.0045\n",
      "Epoch 418/500, Loss: 0.0042\n",
      "Epoch 419/500, Loss: 0.0039\n",
      "Epoch 420/500, Loss: 0.0042\n",
      "Epoch 421/500, Loss: 0.0042\n",
      "Epoch 422/500, Loss: 0.0038\n",
      "Epoch 423/500, Loss: 0.0041\n",
      "Epoch 424/500, Loss: 0.0039\n",
      "Epoch 425/500, Loss: 0.0039\n",
      "Epoch 426/500, Loss: 0.0039\n",
      "Epoch 427/500, Loss: 0.0038\n",
      "Epoch 428/500, Loss: 0.0038\n",
      "Epoch 429/500, Loss: 0.0037\n",
      "Epoch 430/500, Loss: 0.0039\n",
      "Epoch 431/500, Loss: 0.0043\n",
      "Epoch 432/500, Loss: 0.0040\n",
      "Epoch 433/500, Loss: 0.0040\n",
      "Epoch 434/500, Loss: 0.0037\n",
      "Epoch 435/500, Loss: 0.0042\n",
      "Epoch 436/500, Loss: 0.0046\n",
      "Epoch 437/500, Loss: 0.0041\n",
      "Epoch 438/500, Loss: 0.0042\n",
      "Epoch 439/500, Loss: 0.0041\n",
      "Epoch 440/500, Loss: 0.0039\n",
      "Epoch 441/500, Loss: 0.0041\n",
      "Epoch 442/500, Loss: 0.0039\n",
      "Epoch 443/500, Loss: 0.0041\n",
      "Epoch 444/500, Loss: 0.0038\n",
      "Epoch 445/500, Loss: 0.0040\n",
      "Epoch 446/500, Loss: 0.0037\n",
      "Epoch 447/500, Loss: 0.0038\n",
      "Epoch 448/500, Loss: 0.0038\n",
      "Epoch 449/500, Loss: 0.0037\n",
      "Epoch 450/500, Loss: 0.0037\n",
      "Epoch 451/500, Loss: 0.0037\n",
      "Epoch 452/500, Loss: 0.0036\n",
      "Epoch 453/500, Loss: 0.0037\n",
      "Epoch 454/500, Loss: 0.0036\n",
      "Epoch 455/500, Loss: 0.0036\n",
      "Epoch 456/500, Loss: 0.0036\n",
      "Epoch 457/500, Loss: 0.0035\n",
      "Epoch 458/500, Loss: 0.0036\n",
      "Epoch 459/500, Loss: 0.0035\n",
      "Epoch 460/500, Loss: 0.0035\n",
      "Epoch 461/500, Loss: 0.0034\n",
      "Epoch 462/500, Loss: 0.0034\n",
      "Epoch 463/500, Loss: 0.0035\n",
      "Epoch 464/500, Loss: 0.0033\n",
      "Epoch 465/500, Loss: 0.0034\n",
      "Epoch 466/500, Loss: 0.0035\n",
      "Epoch 467/500, Loss: 0.0036\n",
      "Epoch 468/500, Loss: 0.0040\n",
      "Epoch 469/500, Loss: 0.0038\n",
      "Epoch 470/500, Loss: 0.0038\n",
      "Epoch 471/500, Loss: 0.0034\n",
      "Epoch 472/500, Loss: 0.0035\n",
      "Epoch 473/500, Loss: 0.0038\n",
      "Epoch 474/500, Loss: 0.0033\n",
      "Epoch 475/500, Loss: 0.0036\n",
      "Epoch 476/500, Loss: 0.0039\n",
      "Epoch 477/500, Loss: 0.0034\n",
      "Epoch 478/500, Loss: 0.0039\n",
      "Epoch 479/500, Loss: 0.0039\n",
      "Epoch 480/500, Loss: 0.0034\n",
      "Epoch 481/500, Loss: 0.0039\n",
      "Epoch 482/500, Loss: 0.0039\n",
      "Epoch 483/500, Loss: 0.0037\n",
      "Epoch 484/500, Loss: 0.0037\n",
      "Epoch 485/500, Loss: 0.0036\n",
      "Epoch 486/500, Loss: 0.0037\n",
      "Epoch 487/500, Loss: 0.0035\n",
      "Epoch 488/500, Loss: 0.0036\n",
      "Epoch 489/500, Loss: 0.0035\n",
      "Epoch 490/500, Loss: 0.0035\n",
      "Epoch 491/500, Loss: 0.0035\n",
      "Epoch 492/500, Loss: 0.0034\n",
      "Epoch 493/500, Loss: 0.0035\n",
      "Epoch 494/500, Loss: 0.0034\n",
      "Epoch 495/500, Loss: 0.0035\n",
      "Epoch 496/500, Loss: 0.0033\n",
      "Epoch 497/500, Loss: 0.0033\n",
      "Epoch 498/500, Loss: 0.0032\n",
      "Epoch 499/500, Loss: 0.0033\n",
      "Epoch 500/500, Loss: 0.0032\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BasicTransformer(input_dim=inputs.shape[2]).to(device)\n",
    "inputs_train = inputs[:, :100, :].to(device)  # limit sequence length to 100\n",
    "targets_train = targets.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs_train)\n",
    "    loss = criterion(outputs, targets_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fc9624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output for one datum: [[ 1.44911520e-02 -1.84102997e-01  5.74414432e-02 -3.20142359e-02\n",
      "   2.61188000e-02  3.46162580e-02  2.96733826e-02  5.29220849e-02\n",
      "   1.43081948e-01 -3.20722871e-02  1.66428193e-01 -5.60479611e-02\n",
      "  -2.62854341e-02  5.04216813e-02  1.29039630e-01 -3.56406718e-03\n",
      "  -2.34667771e-02 -1.00048847e-01 -1.09580860e-01 -1.15351416e-01\n",
      "  -3.05411182e-02 -2.61618137e-01  8.49955678e-02 -5.30369245e-02\n",
      "   5.85940480e-03 -8.46836120e-02 -7.85928816e-02  1.34819940e-01\n",
      "   2.49203712e-01  5.35501428e-02 -2.54595205e-02  1.33899093e-01\n",
      "   1.37564927e-01  2.29550153e-02 -1.12142287e-01  1.03828117e-01\n",
      "   8.26263428e-03  6.64909780e-02 -2.67749988e-02  8.84292424e-02\n",
      "   1.47625655e-01  2.58991010e-02  1.31726772e-01  5.48251495e-02\n",
      "  -9.43736359e-03  7.53351599e-02  7.45906681e-03  1.90244727e-02\n",
      "   1.06093790e-02 -4.46383730e-02 -1.33795843e-01  8.92596692e-02\n",
      "   5.15896082e-02  3.05640437e-02  1.00529775e-01  1.87410697e-01\n",
      "   5.01280092e-02  1.09381601e-03  1.61742419e-02  5.45810759e-02\n",
      "  -1.68893203e-01 -4.10567187e-02  1.64506927e-01 -1.29116461e-01\n",
      "  -1.17438987e-01 -7.91621357e-02  1.18704550e-01  5.66945821e-02\n",
      "   3.40373442e-03 -5.79720177e-03 -1.52600184e-02 -1.44255474e-01\n",
      "   6.20581582e-02  1.48122609e-01  5.15048318e-02 -1.12036660e-01\n",
      "   1.60545781e-01 -1.15454830e-02  4.17144038e-04 -6.35183528e-02\n",
      "  -1.60020486e-01 -1.08266920e-01 -9.19274241e-02 -1.52209416e-01\n",
      "   1.91710517e-03  1.70059323e-01  1.57042399e-01  1.79613411e-01\n",
      "  -8.28958955e-03 -1.03854030e-01 -5.32018319e-02 -7.85435736e-02\n",
      "  -1.14436328e-01 -2.17341371e-02 -1.90375626e-01  1.43078327e-01\n",
      "   9.15157329e-03 -2.84651339e-01  1.41157228e-02  6.44708350e-02\n",
      "   6.99148327e-03  3.46253902e-01  8.43881071e-02 -6.69188946e-02\n",
      "   1.28992796e-01  3.53347287e-02 -1.93132162e-01 -2.46540889e-01\n",
      "  -1.45115241e-01  8.21770877e-02  1.08424529e-01 -3.02116517e-02\n",
      "   2.54634142e-01  4.19917032e-02 -5.15435934e-02 -9.27936584e-02\n",
      "   1.20340437e-01  1.01709567e-01  1.05395690e-02  5.41802198e-02\n",
      "   5.04659563e-02  2.01659128e-02  1.73418909e-01 -3.82680967e-02\n",
      "  -8.67296010e-02  4.82693501e-02 -5.16221076e-02 -8.80454183e-02\n",
      "  -9.82643142e-02 -1.30488634e-01 -2.96973623e-02  1.68876946e-01\n",
      "  -5.90065867e-03 -1.64341331e-02 -8.94022360e-03  1.28381521e-01\n",
      "   7.01267347e-02  1.54855438e-02  5.82687184e-03 -4.02554721e-02\n",
      "  -1.45812392e-01 -1.32552534e-02  9.36838090e-02  1.18368901e-01\n",
      "  -2.51799941e-01  1.67645514e-04  1.46237284e-01 -2.00623140e-01\n",
      "   1.10891804e-01 -8.99953768e-04  6.15263134e-02  1.60448000e-01\n",
      "  -8.78420994e-02 -7.42448121e-03 -3.54289636e-02 -5.08970022e-02\n",
      "   8.04048125e-03  1.27315834e-01 -4.66077998e-02  1.41507685e-01\n",
      "  -1.73480064e-03  1.48373812e-01 -5.34477569e-02  7.08443001e-02\n",
      "  -6.65884018e-02  2.10638702e-01 -5.59806488e-02 -1.04238652e-01\n",
      "  -1.53193250e-03 -6.08473159e-02  2.69277617e-02 -9.83110815e-02\n",
      "   4.54886928e-02  9.90191475e-03 -5.89796901e-02 -4.70769443e-02\n",
      "   2.90134437e-02 -1.74877550e-02 -5.36340885e-02 -1.09398559e-01\n",
      "  -7.09487572e-02  1.09381601e-02 -1.71945676e-01 -1.47695124e-01\n",
      "  -8.89399275e-02 -7.52707049e-02  6.83539882e-02 -2.30731368e-02\n",
      "   6.74094111e-02 -1.45003900e-01 -6.70433342e-02  3.37938905e-01\n",
      "  -1.92302585e-01 -5.29261231e-02  2.86273584e-02  6.78567365e-02\n",
      "  -2.18501151e-01  8.28909725e-02 -5.06970547e-02  2.37104930e-02\n",
      "  -1.42678246e-01  2.20414381e-02 -2.43357718e-02 -1.73255056e-01\n",
      "  -3.30971368e-02 -1.65633082e-01 -1.12179816e-01  6.40349388e-02\n",
      "   9.07948315e-02 -3.49480063e-02  2.29848549e-01 -6.17177747e-02\n",
      "   1.19164385e-01 -1.20108560e-01 -2.52158940e-03  1.95002072e-02\n",
      "  -1.58799142e-01  2.13129461e-01 -1.47433698e-01  6.55416772e-03\n",
      "  -3.76288742e-02 -1.41862333e-01 -6.52621686e-03  5.19009531e-02\n",
      "   1.11405730e-01  2.82123014e-02 -6.73197210e-02  1.35383859e-01\n",
      "  -5.11956960e-03  1.27642438e-01 -4.78362739e-02  4.59459312e-02\n",
      "   2.17209131e-01  1.24836452e-01  5.27395569e-02  6.62054941e-02\n",
      "  -6.58855811e-02  3.39458212e-02  7.38121197e-03 -1.12416446e-01\n",
      "  -2.17146687e-02 -7.78570846e-02  9.99785028e-03  4.70819660e-02\n",
      "  -1.53270364e-02 -8.33230168e-02 -6.98680654e-02  1.22830436e-01\n",
      "  -5.46492413e-02 -9.08559188e-03 -9.42493603e-02  4.96776439e-02\n",
      "   2.98163556e-02  4.59824502e-02  1.10924982e-01  4.47416268e-02]]\n",
      "Output size: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    single_input = inputs[0, :100, :].unsqueeze(0).to(device)  # shape: (1, 100, input_dim)\n",
    "    output = model(single_input)\n",
    "print(\"Model output for one datum:\", output.cpu().numpy())\n",
    "print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f49451",
   "metadata": {},
   "source": [
    "Convert that output back into a card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9721ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adaptive~Sporesinger', 'Against~All~Odds', 'All~Will~Be~One', 'Ambulatory~Edifice', 'Annex~Sentry', 'Annihilating~Glare', 'Anoint~with~Affliction', 'Apostle~of~Invasion', 'Archfiend~of~the~Dross', 'Argentum~Masticore', 'Armored~Scrapgorger', \"Aspirant's~Ascent\", 'Atmosphere~Surgeon', \"Atraxa's~Skitterfang\", 'Atraxa,~Grand~Unifier', 'Awaken~the~Sleeper', 'Axiom~Engraver', 'Barbed~Batterfist', 'Basilica~Shepherd', 'Basilica~Skullbomb', 'Bilious~Skulldweller', 'Blackcleave~Cliffs', \"Black~Sun's~Twilight\", 'Bladed~Ambassador', 'Bladegraft~Aspirant', 'Bladehold~War-Whip', 'Blade~of~Shared~Souls', 'Blazing~Crescendo', 'Blightbelly~Rat', 'Bloated~Contaminator', \"Blue~Sun's~Twilight\", 'Bonepicker~Skirge', 'Branchblight~Stalker', 'Bring~the~Ending', 'Cacophony~Scamp', 'Cankerbloom', 'Capricious~Hellraiser', 'Carnivorous~Canopy', 'Cephalopod~Sentry', 'Charforger', 'Charge~of~the~Mites', 'Chimney~Rabble', 'Chittering~Skitterling', 'Chrome~Prowler', 'Churning~Reservoir', 'Cinderslash~Ravager', 'Compleat~Devotion', 'Conduit~of~Worlds', 'Contagious~Vorrac', 'Copperline~Gorge', 'Copper~Longlegs', 'Crawling~Chorus', 'Cruel~Grimnarch', 'Cutthroat~Centurion', 'Darkslick~Shores', 'Distorted~Curiosity', 'Dragonwing~Glider', 'Drivnod,~Carnage~Dominus', 'Dross~Skullbomb', 'Drown~in~Ichor', 'Duelist~of~Deep~Faith', 'Dune~Mover', 'Duress', 'Elesh~Norn,~Mother~of~Machines', 'Encroaching~Mycosynth', 'Escaped~Experiment', 'Evolved~Spinoderm', 'Evolving~Adaptive', 'Expand~the~Sphere', 'Experimental~Augury', 'Exuberant~Fuseling', 'Eye~of~Malcator', 'Ezuri,~Stalker~of~Spheres', 'Feed~the~Infection', 'Flensing~Raptor', 'Fleshless~Gladiator', 'Font~of~Progress', 'Forest', 'Forgehammer~Centurion', 'Free~from~Flesh', 'Furnace~Punisher', 'Furnace~Skullbomb', 'Furnace~Strider', 'Geth,~Thane~of~Contracts', 'Gitaxian~Anatomist', 'Gitaxian~Raptor', 'Gleeful~Demolition', 'Glissa~Sunslayer', 'Glistener~Seer', \"Goldwarden's~Helm\", 'Graaz,~Unstoppable~Juggernaut', \"Green~Sun's~Twilight\", 'Gulping~Scraptrap', 'Hazardous~Blast', 'Hexgold~Halberd', 'Hexgold~Hoverwings', 'Hexgold~Slash', 'Ichormoon~Gauntlet', 'Ichorplate~Golem', 'Ichorspit~Basilisk', 'Ichor~Synthesizer', 'Incisor~Glider', 'Incubation~Sac', 'Indoctrination~Attendant', 'Infectious~Bite', 'Infectious~Inquiry', 'Infested~Fleshcutter', 'Island', 'Jace,~the~Perfected~Mind', 'Jawbone~Duelist', 'Jin-Gitaxias,~Progress~Tyrant', 'Jor~Kadeen,~First~Goldwarden', 'Kaito,~Dancing~Shadow', 'Karumonix,~the~Rat~King', 'Kaya,~Intangible~Slayer', 'Kemba,~Kha~Enduring', 'Kethek,~Crucible~Goliath', 'Koth,~Fire~of~Resistance', 'Kuldotha~Cackler', 'Lattice-Blade~Mantis', 'Leonin~Lightbringer', 'Lukka,~Bound~to~Ruin', 'Magmatic~Sprinter', \"Malcator's~Watcher\", 'Malcator,~Purity~Overseer', 'Mandible~Justiciar', \"Maze's~Mantle\", 'Maze~Skullbomb', 'Meldweb~Curator', 'Meldweb~Strider', 'Melira,~the~Living~Cure', 'Mercurial~Spelldancer', 'Mesmerizing~Dose', 'Migloz,~Maze~Crusher', 'Mindsplice~Apparatus', 'Minor~Misstep', 'Mirran~Bardiche', 'Mirrex', 'Molten~Rebuke', 'Mondrak,~Glory~Dominus', 'Monument~to~Perfection', 'Mountain', 'Myr~Convert', 'Myr~Custodian', 'Myr~Kinsmith', \"Nahiri's~Sacrifice\", 'Nahiri,~the~Unforgiving', 'Necrogen~Communion', 'Necrogen~Rotpriest', 'Necrosquito', 'Nimraiser~Paladin', 'Nissa,~Ascended~Animist', \"Norn's~Wellspring\", 'Noxious~Assault', 'Offer~Immortality', 'Oil-Gorger~Troll', 'Orthodoxy~Enforcer', 'Ossification', 'Ovika,~Enigma~Goliath', 'Oxidda~Finisher', 'Paladin~of~Predation', 'Pestilent~Syphoner', 'Phyrexian~Arena', 'Phyrexian~Atlas', 'Phyrexian~Obliterator', 'Phyrexian~Vindicator', 'Plague~Nurse', 'Plains', 'Planar~Disruption', 'Plated~Onslaught', 'Porcelain~Zealot', 'Predation~Steward', 'Prologue~to~Phyresis', 'Prophetic~Prism', 'Prosthetic~Injector', 'Quicksilver~Fisher', 'Ravenous~Necrotitan', 'Razorverge~Thicket', 'Rebel~Salvo', \"Red~Sun's~Twilight\", 'Reject~Imperfection', 'Resistance~Reunited', 'Resistance~Skywarden', 'Ria~Ivor,~Bane~of~Bladehold', 'Ribskiff', 'Rustvine~Cultivator', 'Ruthless~Predation', 'Sawblade~Scamp', 'Scheming~Aspirant', 'Seachrome~Coast', 'Serum-Core~Chimera', 'Serum~Snare', \"Sheoldred's~Edict\", \"Sheoldred's~Headcleaver\", 'Sheoldred,~the~Apocalypse', 'Shrapnel~Slinger', 'Sinew~Dancer', \"Skrelv's~Hive\", 'Skrelv,~Defector~Mite', 'Skyscythe~Engulfer', 'Slaughter~Singer', 'Slobad,~Iron~Goblin', 'Solphim,~Mayhem~Dominus', 'Soulless~Jailer', 'Staff~of~Compleation', 'Stinging~Hivemaster', 'Surgical~Skullbomb', 'Swamp', 'Swooping~Lookout', 'Sword~of~Forge~and~Frontier', 'Sylvok~Battle-Chair', 'Tablet~of~Compleation', 'Tainted~Observer', \"Tamiyo's~Immobilizer\", \"Tamiyo's~Logbook\", 'Tekuthal,~Inquiry~Dominus', 'Terramorphic~Expanse', 'Testament~Bearer', 'The~Autonomous~Furnace', 'The~Dross~Pits', 'The~Eternal~Wanderer', 'The~Fair~Basilica', 'The~Filigree~Sylex', 'The~Hunter~Maze', 'The~Monumental~Facade', 'The~Mycosynth~Gardens', 'The~Seedcore', 'The~Surgical~Bay', 'Thirsting~Roots', 'Thrill~of~Possibility', 'Thrummingbird', 'Thrun,~Breaker~of~Silence', 'Titanic~Growth', 'Transplant~Theorist', 'Trawler~Drake', 'Tyrranax~Atrocity', 'Tyrranax~Rex', \"Tyvar's~Stand\", 'Tyvar,~Jubilant~Brawler', \"Unctus's~Retrofitter\", 'Unctus,~Grand~Metatect', 'Unnatural~Restoration', \"Urabrask's~Anointer\", \"Urabrask's~Forge\", 'Urabrask,~Heretic~Praetor', 'Vanish~into~Eternity', 'Vat~Emergence', 'Vat~of~Rebirth', 'Veil~of~Assimilation', 'Venerated~Rotpriest', 'Venomous~Brutalizer', 'Venser,~Corpse~Puppet', 'Vindictive~Flamestoker', 'Viral~Spawning', 'Vivisection~Evangelist', \"Vivisurgeon's~Insight\", 'Voidwing~Hybrid', 'Volt~Charge', 'Vorinclex,~Monstrous~Raider', 'Vraan,~Executioner~Thane', \"Vraska's~Fall\", \"Vraska,~Betrayal's~Sting\", 'Vulshok~Splitter', 'Watchful~Blisterzoa', 'Whisper~of~the~Dross', \"White~Sun's~Twilight\", \"Zealot's~Conviction\", 'Zenith~Chronicler', 'Zopandrel,~Hunger~Dominus']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load embeddings CSV\n",
    "embeddings_path = 'C:/Users/shwes/Projects/ML/mtg deckbuilding/New/MTG-card2vec/embeddings/ONE/ONE_embeddings.csv'\n",
    "embeddings_df = pd.read_csv(embeddings_path, index_col=0)\n",
    "\n",
    "# Create card_embeddings dict: {card_name: torch.tensor(embedding_vector)}\n",
    "card_embeddings = {\n",
    "    card: torch.tensor(embeddings_df.loc[card].values, dtype=torch.float32)\n",
    "    for card in embeddings_df.index\n",
    "}\n",
    "# print(len(card_embeddings))\n",
    "# print(len(next(iter(card_embeddings.values()))))\n",
    "print(sorted(list(card_embeddings.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3f944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest card: Armored~Scrapgorger\n",
      "Cosine similarity: 0.9148054122924805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find the nearest card embedding to the model output\n",
    "output_vec = output.squeeze().cpu()  # shape: (256,)\n",
    "embedding_matrix = torch.tensor(embeddings_df.values, dtype=torch.float32)  # shape: (num_cards, 256)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = torch.nn.functional.cosine_similarity(output_vec.unsqueeze(0), embedding_matrix)\n",
    "nearest_idx = torch.argmax(cos_sim).item()\n",
    "nearest_card = embeddings_df.index[nearest_idx]\n",
    "\n",
    "print(\"Nearest card:\", nearest_card)\n",
    "print(\"Cosine similarity:\", cos_sim[nearest_idx].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
